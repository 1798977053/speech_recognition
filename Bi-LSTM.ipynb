{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入相关依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa  \n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 该数据集真实场景的标签——12类\n",
    "\n",
    "    Google speech commands dataset. Only 'yes', 'no', 'up', 'down', 'left',\n",
    "        'right', 'on', 'off', 'stop' and 'go' are treated as known classes.\n",
    "        All other classes are used as 'unknown' samples.\n",
    "        See for more information: https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES = 'unknown, silence, yes, no, up, down, left, right, on, off, stop, go'.split(', ')\n",
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 基于FBANK特征提取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbank_features(audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=None)  # 假设'noisy_speech.wav'是您的含噪语音文件  \n",
    "  \n",
    "    # 预处理（如果需要的话，可以在这里加入降噪步骤）  \n",
    "    # 提取FBANK特征  \n",
    "    n_fft = 2048  # FFT窗口大小  \n",
    "    hop_length = 256  # 帧移  \n",
    "    n_mels = 40  # Mel滤波器的数量  \n",
    "    fmin = 0.0  # 最低频率  \n",
    "    fmax = sr / 2  # 最高频率  \n",
    "\n",
    "    # 使用librosa提取FBANK特征  \n",
    "    fbank = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)  \n",
    "    fbank_db = librosa.power_to_db(fbank, ref=np.max)  # 转换为分贝值  \n",
    "    return fbank_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 定义Torch 数据集\n",
    "    加入 silence_percentage=0.1 比例的安静数据集，tricks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_Commands_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, folder, transform=None, classes=CLASSES, silence_percentage=0.2):\n",
    "        all_classes = [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d)) and not d.startswith('_')]\n",
    "\n",
    "\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        for c in all_classes:\n",
    "            if c not in class_to_idx:\n",
    "                class_to_idx[c] = 0\n",
    "\n",
    "        data = []\n",
    "        for c in all_classes:\n",
    "            d = os.path.join(folder, c)\n",
    "            target = class_to_idx[c]\n",
    "            for f in os.listdir(d):\n",
    "                path = os.path.join(d, f)\n",
    "                data.append((path, target))\n",
    "\n",
    "        # add silence\n",
    "        target = class_to_idx['silence']\n",
    "        data += [('', target)] * int(len(data) * silence_percentage)\n",
    "\n",
    "        self.classes = classes\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.data[index]\n",
    "        data = {'path': path, 'target': target}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data,target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 torch Dataset 中的数据转化\n",
    "    需要提取特征维度，这里最好是32维，经验参数，40*32  而非 40*44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_trans(data):\n",
    "    file_path = data['path']\n",
    "    try:\n",
    "        return fbank_features(file_path)[:,:16]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return np.zeros((40,16),dtype=np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 根据 自定义的Dataset创建Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Speech_Commands_Dataset('./train',transform=data_trans)\n",
    "valid_ds = Speech_Commands_Dataset('./valid',transform=data_trans)\n",
    "test_ds = Speech_Commands_Dataset('./test',transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_ds,\n",
    "                                num_workers =0,\n",
    "                                batch_size = 128,\n",
    "                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = torch.utils.data.DataLoader(valid_ds,\n",
    "                                num_workers =0,\n",
    "                                batch_size = 128,\n",
    "                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_ds,\n",
    "                                num_workers =0,\n",
    "                                batch_size = 128,\n",
    "                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bi-LSTM 分类预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_LSTM(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Bi_LSTM,self).__init__()\n",
    "        \n",
    "\n",
    "        self.bi_lstm = nn.LSTM(input_size=16,\n",
    "                               hidden_size=16,\n",
    "                               num_layers = 1,\n",
    "                               batch_first = True,\n",
    "                               bidirectional = True\n",
    "                              )\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(*[\n",
    "            \n",
    "            torch.nn.BatchNorm1d(640),\n",
    "            torch.nn.Linear(640, 128),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Linear(128, num_classes),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            nn.Softmax()\n",
    "\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.flatten(1)\n",
    "       \n",
    "        out = self.predict_layer(x)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|                                                                       | 0/479 [00:00<?, ?it/s]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_236220\\3020874635.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_file, sr=None)  # 假设'noisy_speech.wav'是您的含噪语音文件\n",
      "D:\\worksoft\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss: 2.0517, Val Loss: 1.9365, Val Accuracy: 68.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: Train Loss: 2.0098, Val Loss: 1.9388, Val Accuracy: 68.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# LeNet 对于 minist的参数基本是固定的 ，需要做的说设置迭代次数\n",
    "\n",
    "net = Bi_LSTM(num_classes=len(CLASSES)).to(device) # 初始化一个net\n",
    "criterion = nn.CrossEntropyLoss() # 定义交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "# Train the network 训练网络\n",
    "num_epochs = 2  # 迭代次数\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train step\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataloader, mininterval=2, desc='  - (Training)   ', leave=False):\n",
    "        \n",
    "        data, target = batch\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(data)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(valid_dataloader, mininterval=2, desc='  - (Training)   ', leave=False):\n",
    "            \n",
    "            data, target = batch\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "        \n",
    "\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "            total += target.size(0)\n",
    "        \n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "    val_loss = val_loss / len(valid_dataloader)\n",
    "    val_loss_history.append(val_loss)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/100: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy :.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|                                                                        | 0/65 [00:00<?, ?it/s]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_236220\\3020874635.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_file, sr=None)  # 假设'noisy_speech.wav'是您的含噪语音文件\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9381, Test Accuracy: 68.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "# confusion_matrix = torch.zeros(10, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_dataloader, mininterval=2, desc='  - (Training)   ', leave=False):\n",
    "        data, target = batch\n",
    "        \n",
    "            \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        outputs = net(data)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        \n",
    "        correct += (predicted == target).sum().item()\n",
    "        test_loss += loss.item()\n",
    "#         for i, j in zip(predicted, labels):\n",
    "#             confusion_matrix[i][j] += 1\n",
    "test_loss = test_loss / len(test_dataloader)\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy :.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "# torch.save(net.state_dict(), \"best_model_lenet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.NLLLoss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
